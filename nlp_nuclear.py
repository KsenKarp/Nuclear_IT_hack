# -*- coding: utf-8 -*-
"""NLP_Nuclear.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-XLlF-llDGo61Hy0NAI6ZOim36Je3ef
"""

! pip install spacy

from google.colab import drive
drive.mount('/content/drive')

! python -m spacy download ru_core_news_lg
# Большая русская модель

! pip install transliterate
!pip install langdetect
!pip install deep_translator

! pip install googletrans==4.0.0-rc1

import spacy
import pandas as pd
import transliterate
from deep_translator import GoogleTranslator
import langdetect
import re
import matplotlib.pyplot as plt
import numpy as np
from wordcloud import WordCloud

# Подгружаем файлик с ответами и русскоязычную модель
# и сейчас будем чистить его и всё такое

nlp = spacy.load("ru_core_news_lg")
df = pd.read_csv('/content/answers2.csv', sep=';', header=None, names=['answers'])

# Чтобы перевести всё на русский есть два основных варианта: сервисы от Google
# и сервисы от Яндекса. Яндекс чуть быстрее и лучше работает с русским, НО
# у них нет бесплатного api хотя бы на небольшое количетсво переводимых слов,
# поэтому смотриим на сервисы от Google. Тут можно либо использовать облачный
# перевод, либо ml библиотеку. Попробуем со второй :)


def detect_language(text):
    words = re.findall(r'\b\w+\b', text)
    languages = [langdetect.detect(word) for word in words]
    en_count = languages.count('en')
    if en_count >= 1:
        return 'en'
    else:
        return 'ru'

languages = ['en' if detect_language(answer) == 'en' else 'ru' for answer in df['answers']]

translator = GoogleTranslator(source='en', target='ru')
df['translated_ans'] = df['answers']

for i, language in enumerate(zip(df['answers'], languages)):
    if language == 'en':
        df['translated_ans'][i] = translator.translate( df['translated_ans'][i])

print( df['translated_ans'])

# Можно ещё добавить обработку транслита

def recognize_transliteration(text):
  transliterated_text = transliterate.translit(text, 'ru')
  return transliterated_text

df['translated_ans'] = df['translated_ans'].apply(recognize_transliteration)
print(df['translated_ans'])

# Теперь почистим от эмодзи, знаков препинания и всего такого лишнего

def remove_punctuation(text):
    return re.sub(r'[^\w\sа-яА-ЯёЁ]+', '', text)

df['translated_ans'] = df['translated_ans'].apply(remove_punctuation)
print(df['translated_ans'])

'''
# А ещё хорошо бы учесть всю мощь русского языка и забанить ответы тех,
# кто решил к этой мощи прибегнуть :)
# Ну или перевести на русский литературный ;)

def detect_curse_words(text, curse_words):
    doc = nlp(text)

    # Remove stop words and lemmatize the tokens
    tokens = [token.lemma_ for token in doc if token.is_alpha and token.pos_ != "STOP"]

    # Check if any of the tokens match the curse words
    return any(token in curse_words for token in tokens)

# Example list of curse words in Russian
curse_words = []
'''

# Попробуем подсократить длинные фразы и достать из них главное

def extract_main_word(phrase):

    doc = nlp(phrase)
    if len(phrase) > 4:
      # Выделяем корневой элемент в графе зависимостей между словами
      head_word = [token for token in doc if token.dep_ == "ROOT"][0]

      # Если это существительное, то на вский случай получим
      # его 'характеристики' -- описания или дополнения, относящиеся к нему
      # (работа скучная и работа интересная сильно отличаются)

      if head_word.pos_ == "NOUN":
        adjectives = [token for token in doc if token.dep_ == "amod" and
                    token.head == head_word]
        characterizing_nouns = [token for token in doc if token.dep_ in
        ["nsubj", "nmod"] and token.head == head_word and token.pos_ == "NOUN"]
        head_word = head_word.text
        if adjectives:
          adjective = adjectives[0].text
          head_word = head_word + " " + adjective
        elif characterizing_nouns:
          characterizing_noun = characterizing_nouns[0].text
          head_word = head_word + " " + characterizing_noun

      # Если это галгол, то могут быть важны наречия при нём

      elif head_word.pos_ == "VERB":
        adverbs = [token for token in doc if token.dep_ == "advmod " and
                    token.head == head_word]
        head_word = head_word.text
        if adverbs:
          head_word = head_word + " " + adverbs[0].text

      # Отдельно обработаем наречие для случая
      # предложений без сказуемых и подлежащих

      elif head_word.pos_ == 'ADV':
        nouns = [token for token in doc if token.pos_ == "NOUN"]
        head_word = head_word.text
        if nouns:
          head_word = head_word + " " + nouns[0].text

      else:
          head_word = head_word.text

    # Если выражение и так короткое, то оставляем
    else:
      head_word = phrase.text

    return head_word

df['translated_ans_main'] = df['translated_ans'].apply(extract_main_word)
print(df['translated_ans_main'])

# Для подсчёта косинус-меры будет необходимо выделить леммы у всех слов во фразах

def lemmatize_phrase(phrase):
    doc = nlp(phrase)
    lemmatized_words = [token.lemma_ for token in doc]
    return " ".join(lemmatized_words)



# Формируем словарик с группами синонимов

dictionary = {}
threshold = 0.5
all_words = [word.lower() for word in df['translated_ans_main']]

while all_words:

  w = all_words.pop(0) # Чтобы не работать по десять раз с одним и тем же словом
  w_lemmatized = lemmatize_phrase(w)
  v1 = nlp(w_lemmatized).vector

  # Проверить, было ли уже такое слово -- если нет, то добавляем его как ключ
  if w not in dictionary and w not in [item for sublist in dictionary.values() for item in sublist]:
    dictionary[w] = [w]
  # Если да, то возможно это дубликат -- он нам нужен
  else:
    # Ищем ключ, под которым оно лежит в словаре
    key = next((key for key, values in dictionary.items() if w in values), None)
    dictionary[key].append(w)

  # Ищем все похожие по смыслу слова
  similar_words = [w2 for w2 in all_words if np.dot(v1,
    nlp(lemmatize_phrase(w2)).vector) / (np.linalg.norm(v1) *
    np.linalg.norm(nlp(lemmatize_phrase(w2)).vector)) > threshold]
  for w2 in similar_words:
    key = next((key for key, values in dictionary.items() if w in values), None)
    dictionary[key].append(w2)
    all_words.remove(w2)

print(dictionary)

# На всякий случай выгружаем полученный словарик в формате json,
# для возможности проведения с ним дальнейших манипуляций

import json

with open('grouped_dictionary.json', 'w', encoding='utf-8') as f:
    json.dump(dictionary, f, ensure_ascii=False, indent=4)

! pip install wordcloud

keys = list(dictionary.keys())
frequencies = {key: len(dictionary[key]) for key in keys}

wordcloud = WordCloud(width=800, height=500, background_color="white", colormap="magma",
                      relative_scaling=0.6).generate_from_frequencies(frequencies)

plt.figure(figsize=(16, 16))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()